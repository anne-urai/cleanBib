{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Bibliography\n",
    "\n",
    "To goal of this notebook is to clean your `.bib` file to ensure that it only contains references that you have cited in your paper. This cleaned `.bib` will then be used to generate a data table of names that will be used to query the probabilistic gender classifier, [Gender API](https://gender-api.com). \n",
    "\n",
    "The only required file you need is your manuscript's bibliography in `.bib` format. __Your `.bib` must only contain references cited in the manuscript__. Otherwise, the estimated gender proportions will be inaccurate. \n",
    "\n",
    "If you are not using LaTeX, collect and organize only the references you have cited in your manuscript using your reference manager of choice (e.g. Mendeley, Zotero, EndNote, ReadCube, etc.) and export that selected bibliography as a `.bib` file. For those working in LaTeX, we can use an optional `.aux` file to automatically filter your `.bib` to check that it only contains entries which are cited in your manuscript.\n",
    "\n",
    "| Input               | Output                                                                                                                      |\n",
    "|---------------------|-----------------------------------------------------------------------------------------------------------------------------|\n",
    "| **.bib file(s)**    | cleanBib.csv: table of author first names, titles, and .bib keys                                                            |\n",
    "| .aux file (OPTIONAL)| Authors.csv: table of author first names, estimated gender classification, and confidence                                   |\n",
    "| .tex file (OPTIONAL)| yourTexFile_gendercolor.tex: your .tex file modified to compile with references colored by a legend indicating gender pairs (OPTIONAL) |\n",
    "\n",
    "## Import libraries, set paths, check settings\n",
    "\n",
    "### Upload your `.bib` file(s) and optionally an `.aux` file generated from compiling your LaTeX manuscript and your `.tex` file\n",
    "\n",
    "![upload button](img/upload.png)\n",
    "\n",
    "![confirm upload button](img/confirmUpload.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No .tex file found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import bibtexparser\n",
    "from bibtexparser.bparser import BibTexParser\n",
    "import glob\n",
    "import subprocess\n",
    "import os\n",
    "from pybtex.database.input import bibtex\n",
    "import csv\n",
    "from pylatexenc.latex2text import LatexNodes2Text \n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "\n",
    "def checkcites_output(aux_file):\n",
    "    # take in aux file for tex document, return list of citation keys\n",
    "    # that are in .bib file but not in document\n",
    "\n",
    "    result = subprocess.run(['texlua', 'checkcites.lua', aux_file[0]], stdout=subprocess.PIPE)\n",
    "    result = result.stdout.decode('utf-8')\n",
    "    unused_array_raw = result.split('\\n')\n",
    "    # process array of unused references + other output \n",
    "    unused_array_final = list()\n",
    "    for x in unused_array_raw:\n",
    "        if len(x) > 0: # if line is not empty\n",
    "            if x[0] == '-':  # and if first character is a '-', it's a citation key\n",
    "                unused_array_final.append(x[2:]) # truncate '- '            \n",
    "    if \"------------------------------------------------------------------------\" in unused_array_final:\n",
    "        return(result)\n",
    "    else:\n",
    "        return(unused_array_final)\n",
    "\n",
    "\n",
    "def removeMiddleName(line):\n",
    "    arr = line.split()\n",
    "    last = arr.pop()\n",
    "    n = len(arr)\n",
    "    if n == 4:\n",
    "        first, middle = ' '.join(arr[:2]), ' '.join(arr[2:])\n",
    "    elif n == 3:\n",
    "        first, middle = arr[0], ' '.join(arr[1:])\n",
    "    elif n == 2:\n",
    "        first, middle = arr\n",
    "    elif n==1:\n",
    "        return line\n",
    "    return(str(first + ' ' + middle))\n",
    "\n",
    "\n",
    "def convertLatexSpecialChars(latex_text):\n",
    "    return LatexNodes2Text().latex_to_text(latex_text)\n",
    "\n",
    "\n",
    "def convertSpecialCharsToUTF8(text):\n",
    "    data = LatexNodes2Text().latex_to_text(text)\n",
    "    return unicodedata.normalize('NFD', data).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "def namesFromTitle(title, authorPos):\n",
    "    if authorPos == 'first':\n",
    "        idx = 0\n",
    "    elif authorPos == 'last':\n",
    "        idx = -1\n",
    "        \n",
    "    works = cr.works(query = f'title:\"{title}\"', select = [\"title\",\"author\"], cursor_max=1)\n",
    "    # find a way to skip F1000 refs\n",
    "    if works['message']['items'][0]['title'][0].lower() == title.lower():\n",
    "        authors = works['message']['items'][0]['author']\n",
    "        \n",
    "        # check the all fields are available\n",
    "        if not 'given' in authors[idx]:\n",
    "            name = ''\n",
    "        else:\n",
    "            # trim initials\n",
    "            name = authors[idx]['given'].replace('.',' ').split()[0]\n",
    "        \n",
    "        # throw out if full first name not available, avoid querying initials\n",
    "        if len(FA) == 1:\n",
    "            FA = ''\n",
    "        if len(LA) == 1:\n",
    "            LA = ''\n",
    "    else:\n",
    "        name = ''\n",
    "    return name\n",
    "\n",
    "\n",
    "homedir = 'C:\\\\Users\\\\jenis\\\\Documents\\\\cleanBib\\\\'\n",
    "bib_files = glob.glob(homedir + '*.bib')\n",
    "paper_aux_file = glob.glob(homedir + '*.aux')\n",
    "paper_bib_file = 'ecog_methods.bib'\n",
    "try:\n",
    "    tex_file = glob.glob(homedir + \"*.tex\")[0]\n",
    "except:\n",
    "    print('No .tex file found.')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the _first_ and _last_ author of your paper.\n",
    "\n",
    "For example: \n",
    "```\n",
    "yourFirstAuthor = 'Teich, Erin G.'\n",
    "yourLastAuthor = 'Bassett, Danielle S.'\n",
    "```\n",
    "\n",
    "And optionally, define any co-first or co-last author(s), making sure to keep the square brackets to define a list.\n",
    "\n",
    "For example:\n",
    "```\n",
    "optionalEqualContributors = ['Dworkin, Jordan', 'Stiso, Jennifer']\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "optionalEqualContributors = ['Dworkin, Jordan']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Tian\n",
      "3: Dabney2019\n",
      "5: Kucyi\n",
      "7: Zhou  <-- self-citation\n",
      "9: Lachaux2012\n",
      "11: Vidaurre2018\n",
      "13: DeCheveigne2019\n",
      "15: Crone2011\n",
      "17: Goyal2018\n",
      "19: Kramer2011\n",
      "21: Chu2012\n",
      "23: Cole2018\n",
      "25: Voytek2012\n",
      "27: Cole2017a\n",
      "29: Das2017\n",
      "31: Lachaux2002\n",
      "33: Lebedev2016\n",
      "35: Catanese2016\n",
      "37: Tooley2018  <-- self-citation\n",
      "39: Haller2018\n",
      "41: VanderMeij2018\n",
      "43: Pesaran2018\n",
      "45: Akam2014\n",
      "47: Roach2018a\n",
      "49: Fox2018\n",
      "51: Horn2017\n",
      "53: Staba2002\n",
      "55: Buzsaki2004\n",
      "57: Onojima2018\n",
      "59: Rossini2017\n",
      "61: Schnitzler2005\n",
      "63: Buzsaki2012\n",
      "65: Holdgraf2017\n",
      "67: Causality2017\n",
      "69: Peterson2017\n",
      "71: Peterson\n",
      "73: DuprelaTour2017\n",
      "75: Matsui2017\n",
      "77: Bahramisharif2017\n",
      "79: Miller\n",
      "81: Atasoy\n",
      "83: Schalk2017\n",
      "85: Schwalb\n",
      "87: Wiener2017\n",
      "89: Centre2017\n",
      "91: Grossman2017\n",
      "93: Cole2017\n",
      "95: Bruns2004\n",
      "97: ChandranKS2016\n",
      "99: Miller2009\n",
      "101: Gerber2016\n",
      "103: Grashow2009\n",
      "105: Mercier2016\n",
      "107: Klausberger2008\n",
      "109: Cole2016a\n",
      "111: Vinck2011\n",
      "113: UngH\t\t  <--  ***NAME MISSING OR POSSIBLY INCOMPLETE***\n",
      "115: Parvizi2017\n"
     ]
    }
   ],
   "source": [
    "yourFirstAuthor = 'Stiso, Jennifer'\n",
    "yourLastAuthor = 'Bassett, Danielle S'\n",
    "optionalEqualContributors = ['Stiso, Jennifer']\n",
    "\n",
    "if (yourFirstAuthor == 'LastName, FirstName OptionalMiddleInitial') or (yourLastAuthor == 'LastName, FirstName OptionalMiddleInitial'):\n",
    "    raise ValueError(\"Please enter your manuscript's first and last author names\")\n",
    "\n",
    "if paper_aux_file:\n",
    "    if optionalEqualContributors == ('LastName, FirstName OptionalMiddleInitial', 'LastName, FirstName OptionalMiddleInitial'):\n",
    "        citing_authors = np.array([yourFirstAuthor, yourLastAuthor])\n",
    "    else:\n",
    "        citing_authors = np.array([yourFirstAuthor, yourLastAuthor, optionalEqualContributors])\n",
    "    print(checkcites_output(paper_aux_file))\n",
    "    unused_in_paper = checkcites_output(paper_aux_file) # get citations in library not used in paper\n",
    "    print(\"Unused citations: \", unused_in_paper.count('=>'))\n",
    "    \n",
    "    parser = BibTexParser()\n",
    "    parser.ignore_nonstandard_types = False\n",
    "    parser.common_strings = True\n",
    "    \n",
    "    bib_data = None\n",
    "    for bib_file in bib_files:\n",
    "        with open(bib_file) as bibtex_file:\n",
    "            if bib_data is None:\n",
    "                bib_data = bibtexparser.bparser.BibTexParser(common_strings=True, ignore_nonstandard_types=False).parse_file(bibtex_file)\n",
    "                # bib_data = bibtexparser.load(bibtex_file, parser)\n",
    "            else:\n",
    "                bib_data_extra = bibtexparser.bparser.BibTexParser(common_strings=True, ignore_nonstandard_types=False).parse_file(bibtex_file)\n",
    "                # bib_data_extra = bibtexparser.load(bibtex_file, parser)\n",
    "                bib_data.entries_dict.update(bib_data_extra.entries_dict)\n",
    "                bib_data.entries.extend(bib_data_extra.entries)\n",
    "    \n",
    "    all_library_citations = list(bib_data.entries_dict.keys())\n",
    "    print(\"All citations: \", len(all_library_citations))\n",
    "    \n",
    "    for k in all_library_citations:\n",
    "        if k in unused_in_paper:\n",
    "            del bib_data.entries_dict[k] # remove from entries dictionary if not in paper\n",
    "    \n",
    "    #in_paper_mask = [x not in unused_in_paper for x in all_library_citations] # get mask of citations in paper\n",
    "    in_paper_mask = [bib_data.entries[x]['ID'] not in unused_in_paper for x in range(len(bib_data.entries))]\n",
    "    bib_data.entries = [bib_data.entries[x] for x in np.where(in_paper_mask)[0]] # replace entries list with entries only in paper\n",
    "    del bib_data.comments\n",
    "    \n",
    "    duplicates = []\n",
    "    for key in bib_data.entries_dict.keys():\n",
    "        count = str(bib_data.entries).count(key)\n",
    "        if count > 1:\n",
    "            duplicates.append(key)\n",
    "            \n",
    "    if len(duplicates) > 0:\n",
    "        raise ValueError(\"In your .bib file, please remove duplicate entries or duplicate entry ID keys for:\", ' '.join(map(str, duplicates)))\n",
    "\n",
    "    if os.path.exists(paper_bib_file):\n",
    "        os.remove(paper_bib_file)\n",
    "    \n",
    "    with open(paper_bib_file, 'w') as bibtex_file:\n",
    "        bibtexparser.dump(bib_data, bibtex_file)\n",
    "    \n",
    "    # define first author and last author names of citing paper -- will exclude citations of these authors\n",
    "    # beware of latex symbols within author names\n",
    "    # in_paper_citations = list(bib_data.entries_dict.keys())\n",
    "    in_paper_citations = [bib_data.entries[x]['ID'] for x in range(len(bib_data.entries))] # get list of citation keys in paper\n",
    "    \n",
    "    # extract author list for every cited paper\n",
    "    cited_authors = [bib_data.entries_dict[x]['author'] for x in in_paper_citations]\n",
    "    # find citing authors in cited author list\n",
    "    # using nested list comprehension, make a citing author -by- citation array of inclusion\n",
    "    self_cite_mask = np.array([[citing_author in authors for authors in cited_authors] for citing_author in citing_authors])\n",
    "    self_cite_mask = np.any(self_cite_mask,axis=0) # collapse across citing authors such that any coauthorship by either citing author -> exclusion\n",
    "    \n",
    "    print(\"Self-citations: \", [bib_data.entries[x]['ID'] for x in np.where(self_cite_mask)[0]]) # print self citations\n",
    "    for idx,k in enumerate(in_paper_citations):\n",
    "        if self_cite_mask[idx]:\n",
    "            del bib_data.entries_dict[k] # delete citation from dictionary if self citationi\n",
    "    bib_data.entries = [bib_data.entries[x] for x in np.where(np.invert(self_cite_mask))[0]] # replace entries list with entries that aren't self citations\n",
    "    \n",
    "    paper_bib_file_excl_sc = os.path.splitext(paper_bib_file)[0] + '_noselfcite.bib'\n",
    "    \n",
    "    if os.path.exists(paper_bib_file_excl_sc):\n",
    "        os.remove(paper_bib_file_excl_sc)\n",
    "    \n",
    "    with open(paper_bib_file_excl_sc, 'w') as bibtex_file:\n",
    "        bibtexparser.dump(bib_data, bibtex_file)\n",
    "        \n",
    "if os.path.exists('*_noselfcite.bib'):\n",
    "    ID = glob.glob(homedir + paper_bib_file_excl_sc)\n",
    "else:\n",
    "    ID = glob.glob(homedir + '*bib')\n",
    "    with open(ID[0]) as bibtex_file:\n",
    "        bib_data = bibtexparser.bparser.BibTexParser(common_strings=True, ignore_nonstandard_types=False).parse_file(bibtex_file)\n",
    "    duplicates = []\n",
    "    for key in bib_data.entries_dict.keys():\n",
    "        count = str(bib_data.entries).count(\"'ID\\': \\'\"+ key + \"\\'\")\n",
    "        if count > 1:\n",
    "            duplicates.append(key)\n",
    "            \n",
    "    if len(duplicates) > 0:\n",
    "        raise ValueError(\"In your .bib file, please remove duplicate entries or duplicate entry ID keys for:\", ' '.join(map(str, duplicates)))\n",
    "\n",
    "FA = []\n",
    "LA = []\n",
    "parser = bibtex.Parser()\n",
    "bib_data = parser.parse_file(ID[0])\n",
    "counter = 1\n",
    "nameCount = 0\n",
    "outPath = homedir + 'cleanedBib.csv'\n",
    "\n",
    "if os.path.exists(outPath):\n",
    "    os.remove(outPath)\n",
    "\n",
    "with open(outPath, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['Article', 'FA', 'LA', 'Title', 'SelfCite', 'CitationKey'])\n",
    "\n",
    "for key in bib_data.entries.keys():\n",
    "    try:\n",
    "        author = bib_data.entries[key].persons['author']\n",
    "    except:\n",
    "        author = bib_data.entries[key].persons['editor']\n",
    "    FA = author[0].rich_first_names\n",
    "    LA = author[-1].rich_first_names\n",
    "    FA = convertLatexSpecialChars(str(FA)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")\n",
    "    LA = convertLatexSpecialChars(str(LA)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")\n",
    "\n",
    "    if (yourFirstAuthor!='LastName, FirstName OptionalMiddleInitial') and (yourLastAuthor!='LastName, FirstName OptionalMiddleInitial'):\n",
    "        selfCiteCheck1 = [s for s in author if removeMiddleName(yourLastAuthor) in str([convertLatexSpecialChars(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertLatexSpecialChars(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "        selfCiteCheck1a = [s for s in author if removeMiddleName(yourLastAuthor) in str([convertSpecialCharsToUTF8(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertSpecialCharsToUTF8(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "        \n",
    "        selfCiteCheck2 = [s for s in author if removeMiddleName(yourFirstAuthor) in str([convertLatexSpecialChars(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertLatexSpecialChars(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "        selfCiteCheck2a = [s for s in author if removeMiddleName(yourFirstAuthor) in str([convertSpecialCharsToUTF8(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertSpecialCharsToUTF8(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "        nameCount = 0\n",
    "        if optionalEqualContributors != ('LastName, FirstName OptionalMiddleInitial', 'LastName, FirstName OptionalMiddleInitial'):\n",
    "            for name in optionalEqualContributors:\n",
    "                selfCiteCheck3 = [s for s in author if removeMiddleName(name) in str([convertLatexSpecialChars(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertLatexSpecialChars(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "                selfCiteCheck3a = [s for s in author if removeMiddleName(name) in str([convertSpecialCharsToUTF8(str(s.rich_last_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\"), convertSpecialCharsToUTF8(str(s.rich_first_names)[7:-3]).replace(\"', Protected('\",\"\").replace(\"'), '\", \"\")]).replace(\"'\", \"\")]\n",
    "                if len(selfCiteCheck3)>0:\n",
    "                    nameCount += 1\n",
    "                if len(selfCiteCheck3a)>0:\n",
    "                    nameCount += 1\n",
    "        selfCiteChecks = [selfCiteCheck1, selfCiteCheck1a, selfCiteCheck2, selfCiteCheck2a]\n",
    "        if sum([len(check) for check in selfCiteChecks]) + nameCount > 0:\n",
    "            selfCite = 'Y'\n",
    "            if len(FA) < 2:\n",
    "                print(str(counter) + \": \" + key + \"\\t\\t  <-- self-citation <--  ***NAME MISSING OR POSSIBLY INCOMPLETE***\")\n",
    "            else:\n",
    "                print(str(counter) + \": \" + key + \"  <-- self-citation\")\n",
    "        else:\n",
    "            selfCite= 'N'\n",
    "            if len(FA) < 2:\n",
    "                print(str(counter) + \": \" + key + \"\\t\\t  <--  ***NAME MISSING OR POSSIBLY INCOMPLETE***\")\n",
    "            else:\n",
    "                print(str(counter) + \": \" + key)\n",
    "    else:\n",
    "        selfCite = 'NA'\n",
    "\n",
    "    with open(outPath, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        if selfCite=='N':\n",
    "            writer.writerow([counter, FA, LA, bib_data.entries[key].fields['title'].replace(',', ''), selfCite, key])\n",
    "    counter += 1\n",
    "\n",
    "    with open(outPath, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        if selfCite=='N':\n",
    "            writer.writerow([counter, FA, LA, bib_data.entries[key].fields['title'].replace(',', ''), selfCite, key])\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate gender of authors from cleaned bibliography\n",
    "\n",
    "### Checkpoint for cleaned bibliography and using Gender API to estimate genders by first names\n",
    "After registering for a [gender-api](https://gender-api.com/) (free account available), use your 500 free monthly search credits by pasting your API key in the code for the line indicated below:\n",
    "\n",
    "```\n",
    "genderAPI_key <- '&key=YOUR ACCOUNT KEY HERE'\n",
    "```\n",
    "\n",
    "[You can find your key in your account's profile page.](https://gender-api.com/en/account/overview#my-api-key)\n",
    "\n",
    "If any of your cleanBib.csv entries are incomplete or contain first initials, the code will not continue to the stage that will use your limited free credits. Please manually edit the cleanedBib.csv by downloading the file, modifying it, and re-uploading it. Common issues include: bibliography entry did not include a last author because the author list was truncated by \"and Others\" or \"et al.\" Some older journals articles only provide first initial and not full first names, in which case you will need to go digging via Google to identify that person. In rare cases where the author cannot be identified even after searching by hand, replace the first name with \"UNKNOWNNAME\" so that the classifier will estimate the gender as unknown. \n",
    "\n",
    "__NOTE__: your free account has 500 queries per month. This box contains the code that will use your limited API credits/queries if it runs without error. Re-running all code repeatedly will repeatedly use these credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the proportions of genders in your reference list and compare it to published base rates in neuroscience.\n",
    "\n",
    "The output will provide a frequency count for male-male, male-female, female-male, and female-female. Your reference proportions will be displayed next to expected proportions in the field of neuroscience. We print the proportion difference relative to expected proportions for neuroscience.\n",
    "\n",
    "OPTIONALLY: Modify Authors.csv, re-upload your manually modified Authors.csv, uncomment #names<-read.csv('Authors.csv'), and rerun the second box/section. This box does NOT contain code that will use your limited API credits/queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Color-code your .tex file using the estimated gender classifications\n",
    "\n",
    "Running this code-block will optionally output your uploaded `.tex` file with color-coding for gender pair classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite_gender = pd.read_csv(homedir+'Authors.csv') # output of getReferenceGends.ipynb\n",
    "cite_gender.index = cite_gender.CitationKey\n",
    "cite_gender['Color'] = '' # what color to make each gender category\n",
    "colors = {'MM':'red','MW':'blue','WW':'green','WM':'magenta','UU':'black',\n",
    "'MU':'black','UM':'black','UW':'black','WU':'black'}\n",
    "for idx in cite_gender.index: # loop through each citation key and set color\n",
    "    cite_gender.loc[idx,'Color'] = colors[cite_gender.loc[idx,'GendCat']]\n",
    "cite_gender.loc[cite_gender.index[cite_gender.SelfCite=='Y'],'Color'] = 'black' # make self citations black\n",
    "\n",
    "fin = open(homedir+tex_file)\n",
    "texdoc=fin.readlines()\n",
    "with open(homedir+tex_file[:-4]+'_gendercolor.tex','w') as fout:\n",
    "    for i in range(len(texdoc)):\n",
    "        s = texdoc[i]\n",
    "        cite_instances = re.findall('\\\\\\\\cite\\{.*?\\}',s)\n",
    "        cite_keys = re.findall('\\\\\\\\cite\\{(.*?)\\}',s)\n",
    "        cite_keys = [x.split(',') for x in cite_keys]\n",
    "        cite_keys_sub = [['\\\\textcolor{' + cite_gender.loc[x.strip(),'Color'] + '}{\\\\cite{'+x.strip()+'}}' for x in cite_instance] for cite_instance in cite_keys]\n",
    "        cite_keys_sub = ['\\\\textsuperscript{,}'.join(x) for x in cite_keys_sub]\n",
    "        for idx,cite_instance in enumerate(cite_instances):\n",
    "            s = s.replace(cite_instances[idx],cite_keys_sub[idx])\n",
    "        fout.write(s)\n",
    "        # place color key after abstract\n",
    "        if '\\\\section*{Introduction}\\n' in s:            \n",
    "            l = ['\\\\textcolor{' + colors[k] + '}{'+k+'}' for k in colors.keys()]\n",
    "            fout.write('\\tKey: '+ ', '.join(l)+'.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
